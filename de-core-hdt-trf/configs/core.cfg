[paths]
train = null
dev = null
init_tok2vec = null
vectors = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = "de"
pipeline = ["transformer","tagger","morphologizer","trainable_lemmatizer","parser","ner"]
batch_size = 32
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}

[components]

[components.trainable_lemmatizer]
source = "training/dep/model-best"

[components.morphologizer]
source = "training/dep/model-best"

[components.parser]
source = "training/dep/model-best"

[components.tagger]
source = "training/dep/model-best"

[components.transformer]
source = "training/dep/model-best"

[components.ner]
source = "training/ner/model-best"
