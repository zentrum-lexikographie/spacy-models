title: "DWDS NLP Pipeline"
description: >
  This project lets you train a part-of-speech tagger, morphologizer,
  lemmatizer and dependency parser from a HDT corpus.  It takes care
  of data preparation, converting it to spaCy's format and training
  and evaluating the model. Note that multi-word tokens will be merged
  together when the corpus is converted since spaCy does not support
  multi-word token expansion.

vars:
  package_name: "core_hdt_dist"
  package_version: "0.2.0"
  gpu: -1

spacy_version: ">=3.3.0,<4.0.0"

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories: ["assets", "corpus", "training", "metrics", "configs", "packages", "scripts"]

assets:
  - dest: "assets/ud-german-hdt.tar.gz"
    url: "https://codeload.github.com/UniversalDependencies/UD_German-HDT/tar.gz/refs/tags/r2.12"
  - dest: "assets/wikiner-de.bz2"
    url: "https://raw.githubusercontent.com/dice-group/FOX/master/input/Wikiner/aij-wikiner-de-wp3.bz2"

workflows:
  all:
    - preprocess
    - configure-dep
    - label-init-dep
    - train-dep
    - configure-ner
    - train-ner
    - evaluate-dep
    - evaluate-ner
  dep:
    - configure-dep
    - label-init-dep
    - train-dep
  ner:
    - configure-ner
    - train-ner
commands:
  - name: preprocess
    help: "Convert the data to spaCy's format"
    script:
      - >-
        mkdir -p assets/ud-german-hdt corpus/ud-german-hdt corpus/wikiner-de
      - >-
        tar -x -f assets/ud-german-hdt.tar.gz
        --strip-components=1
        -C assets/ud-german-hdt
      - >-
        bash -c 'cat
        assets/ud-german-hdt/de_hdt-ud-train-a-1.conllu
        assets/ud-german-hdt/de_hdt-ud-train-a-2.conllu
        assets/ud-german-hdt/de_hdt-ud-train-b-1.conllu
        assets/ud-german-hdt/de_hdt-ud-train-b-2.conllu
        >corpus/ud-german-hdt/train.conllu'
      - >-
        cp
        assets/ud-german-hdt/de_hdt-ud-dev.conllu
        corpus/ud-german-hdt/dev.conllu
      - >-
        cp
        assets/ud-german-hdt/de_hdt-ud-test.conllu
        corpus/ud-german-hdt/test.conllu
      - >-
        python -m spacy convert
        corpus/ud-german-hdt/train.conllu
        corpus/ud-german-hdt
        --converter conllu
        --n-sents 32
        --merge-subtokens
      - >-
        python -m spacy convert
        corpus/ud-german-hdt/dev.conllu
        corpus/ud-german-hdt
        --converter conllu
        --n-sents 32
        --merge-subtokens
      - >-
        python -m spacy convert
        corpus/ud-german-hdt/test.conllu
        corpus/ud-german-hdt
        --converter conllu
        --n-sents 32
        --merge-subtokens
      - >-
        python scripts/partition.py
        assets/wikiner-de.bz2
        assets/wikiner-de-iob
      - >-
        python -m spacy convert
        assets/wikiner-de-iob
        corpus/wikiner-de
        --n-sents 10
    deps:
      - "assets/ud-german-hdt.tar.gz"
      - "assets/wikiner-de.bz2"
    outputs:
      - "corpus/ud-german-hdt/train.spacy"
      - "corpus/ud-german-hdt/dev.spacy"
      - "corpus/ud-german-hdt/test.spacy"
      - "corpus/wikiner-de/train.spacy"
      - "corpus/wikiner-de/dev.spacy"
      - "corpus/wikiner-de/test.spacy"
  - name: configure-dep
    help: "Create full configuration file."
    script:
      - python -m spacy init fill-config configs/dep-base.cfg configs/dep.cfg
      - python -m spacy init fill-config configs/ner-base.cfg configs/ner.cfg
    deps:
      - "configs/dep-base.cfg"
      - "configs/ner-base.cfg"
    outputs:
      - "configs/dep.cfg"
      - "configs/ner.cfg"
  - name: label-init-dep
    help: "Initializes labels"
    script:
      - >-
        python -m spacy init labels
        --gpu-id ${vars.gpu}
        configs/dep.cfg
        corpus/ud-german-hdt
    deps:
      - "configs/dep.cfg"
      - "corpus/ud-german-hdt/train.spacy"
      - "corpus/ud-german-hdt/dev.spacy"
      - "corpus/ud-german-hdt/test.spacy"
    outputs:
      - "corpus/ud-german-hdt/tagger.json"
      - "corpus/ud-german-hdt/morphologizer.json"
      - "corpus/ud-german-hdt/trainable_lemmatizer.json"
      - "corpus/ud-german-hdt/parser.json"
  - name: train-dep
    help: "Train ${vars.package_name}"
    script:
      - >-
        python -m spacy train
        configs/dep.cfg
        --output training/dep
        --gpu-id ${vars.gpu}
    deps:
      - "corpus/ud-german-hdt/train.spacy"
      - "corpus/ud-german-hdt/dev.spacy"
      - "configs/dep.cfg"
    outputs:
      - "training/dep/model-best"
  - name: train-ner
    help: "Train the full pipeline"
    script:
      - >-
        python -m spacy train
        configs/ner.cfg
        --output training/ner
        --gpu-id ${vars.gpu}
    deps:
      - "corpus/wikiner-de/train.spacy"
      - "corpus/wikiner-de/dev.spacy"
      - "configs/ner.cfg"
    outputs:
      - "training/ner/model-best"
  - name: evaluate-dep
    help: "Evaluate on the test data and save the metrics"
    script:
      - >-
        python -m spacy evaluate
        training/dep/model-best
        corpus/ud-german-hdt/test.spacy
        --output ./metrics/dep.json
        --gpu-id ${vars.gpu}
    deps:
      - "training/dep/model-best"
      - "corpus/ud-german-hdt/test.spacy"
    outputs:
      - "metrics/dep.json"
  - name: evaluate-ner
    help: "Evaluate on the test data and save the metrics"
    script:
      - >-
        python -m spacy evaluate
        ./training/ner/model-best
        ./corpus/wikiner-de/test.spacy
        --output ./metrics/ner.json
        --gpu-id ${vars.gpu}
    deps:
      - "training/ner/model-best"
      - "corpus/wikiner-de/test.spacy"
    outputs:
      - "metrics/ner.json"
  - name: "assemble"
    help: "Assemble all parts into a complete coref pipeline."
    script:
      - spacy assemble configs/core.cfg training/core
    deps:
      - training/dep/model-best
      - training/ner/model-best
      - configs/core.cfg

  - name: package
    help: "Package the trained model so it can be installed"
    script:
      - >-
        python -m spacy package
        training/core packages
        --name ${vars.package_name}
        --version ${vars.package_version}
        --build wheel
        --force
    deps:
      - "training/core"
    outputs_no_cache:
      - "packages/de_${vars.package_name}-${vars.package_version}/dist/de_${vars.package_name}-${vars.package_version}.tar.gz"
  - name: clean
    help: "Remove intermediate files"
    script:
      - "rm -rf training/*"
      - "rm -rf metrics/*"
      - "rm -rf corpus/*"
